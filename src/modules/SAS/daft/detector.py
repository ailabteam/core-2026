"""
Core detection logic cho seal vÃ  signature detection
Sá»­ dá»¥ng ModelManager Ä‘á»ƒ reuse model instance
"""
import time
import torch
from typing import Optional, List, Tuple
from PIL import Image

from .model_manager import ModelManager
from .models import DetectionResult
from .utils import (
    resize_image_if_needed,
    create_detection_prompt,
    parse_response,
    draw_boxes
)


def timer(func):
    """Timer decorator"""
    def wrapper(*args, **kwargs):
        start = time.perf_counter()
        result = func(*args, **kwargs)
        end = time.perf_counter()
        print(f"[TIMER] {func.__name__} took {end - start:.3f} seconds")
        return result
    return wrapper


class SealSignatureDetector:
    """Class Ä‘á»ƒ nháº­n diá»‡n vÃ  Ä‘á»‹nh vá»‹ con dáº¥u vÃ  chá»¯ kÃ½ trong áº£nh"""
    
    def __init__(
        self,
        model_path: str = "deepseek-ai/deepseek-vl2-tiny",
        device: Optional[torch.device] = None,
        max_new_tokens: int = 1024,
        max_image_size: Optional[int] = None,
        low_memory_mode: bool = False,
        use_multi_gpu: bool = True,
        device_map: Optional[str] = None,
    ):
        """
        Khá»Ÿi táº¡o detector
        
        Args:
            model_path: ÄÆ°á»ng dáº«n Ä‘áº¿n model DeepSeek-VL2
            device: Thiáº¿t bá»‹ Ä‘á»ƒ cháº¡y model (cuda/cpu). Náº¿u None thÃ¬ tá»± Ä‘á»™ng phÃ¡t hiá»‡n
            max_new_tokens: Sá»‘ token tá»‘i Ä‘a cho response
            max_image_size: KÃ­ch thÆ°á»›c tá»‘i Ä‘a cá»§a áº£nh (resize náº¿u lá»›n hÆ¡n). None = khÃ´ng resize
            low_memory_mode: Náº¿u True, sáº½ clear cache sau má»—i step vÃ  resize áº£nh tá»± Ä‘á»™ng
            use_multi_gpu: Náº¿u True vÃ  cÃ³ nhiá»u GPU, sáº½ tá»± Ä‘á»™ng phÃ¢n tÃ¡n model trÃªn cÃ¡c GPU
            device_map: Device map strategy ("auto", "balanced", "balanced_low_0", hoáº·c dict)
        """
        self.model_path = model_path
        self.max_new_tokens = max_new_tokens
        self.max_image_size = max_image_size
        self.low_memory_mode = low_memory_mode
        
        # Láº¥y ModelManager instance (singleton)
        self.model_manager = ModelManager()
        
        # Load model náº¿u chÆ°a Ä‘Æ°á»£c load
        if not self.model_manager.is_loaded():
            self.model_manager.load_model(
                model_path=model_path,
                device=device,
                max_image_size=max_image_size,
                low_memory_mode=low_memory_mode,
                use_multi_gpu=use_multi_gpu,
                device_map=device_map,
            )
        
        # Láº¥y cÃ¡c components tá»« model manager
        self.model = self.model_manager.get_model()
        self.processor = self.model_manager.get_processor()
        self.tokenizer = self.model_manager.get_tokenizer()
        self.device = self.model_manager.get_device()
        self.num_gpus = self.model_manager.get_num_gpus()
    
    def _clear_cuda_cache(self):
        """Clear CUDA cache"""
        self.model_manager.clear_cache()
    
    @timer
    def detect(
        self,
        image_path: str,
        language: str = "vietnamese",
        return_image: bool = False,
    ) -> DetectionResult:
        """
        Nháº­n diá»‡n con dáº¥u vÃ  chá»¯ kÃ½ trong áº£nh
        
        Args:
            image_path: ÄÆ°á»ng dáº«n Ä‘áº¿n file áº£nh
            language: NgÃ´n ngá»¯ cá»§a prompt (vietnamese/english)
            return_image: Náº¿u True, tráº£ vá» thÃªm PIL Image Ä‘Ã£ Ä‘Æ°á»£c váº½ bounding boxes
        
        Returns:
            DetectionResult object hoáº·c tuple (DetectionResult, PIL.Image) náº¿u return_image=True
        """
        # Clear cache trÆ°á»›c khi xá»­ lÃ½ (náº¿u low memory mode)
        if self.low_memory_mode:
            self._clear_cuda_cache()
        
        # Load image
        image = Image.open(image_path).convert("RGB")
        
        # Resize áº£nh náº¿u cáº§n
        image = resize_image_if_needed(
            image,
            max_image_size=self.max_image_size,
            low_memory_mode=self.low_memory_mode,
            num_gpus=self.num_gpus
        )
        img_width, img_height = image.size
        
        # Táº¡o prompt
        prompt_text = create_detection_prompt(language=language)
        
        # Táº¡o conversation
        conversation = [
            {
                "role": "<|User|>",
                "content": f"<image>\n{prompt_text}",
                "images": [image],
            },
            {"role": "<|Assistant|>", "content": ""},
        ]
        
        # Process inputs
        model_inputs = self.processor(
            conversations=conversation,
            images=[image],
            force_batchify=True,
            system_prompt="You are an expert in document analysis and object detection."
        )
        
        # Move inputs to device
        dtype = torch.float16 if self.device.type == "cuda" else torch.float32
        
        # XÃ¡c Ä‘á»‹nh device Ä‘á»ƒ move inputs
        input_device = self.device
        if hasattr(self.model, "hf_device_map") and self.model.hf_device_map:
            first_device_value = list(self.model.hf_device_map.values())[0]
            if isinstance(first_device_value, (int, str)):
                if isinstance(first_device_value, int):
                    input_device = torch.device(f"cuda:{first_device_value}")
                else:
                    input_device = torch.device(first_device_value)
            elif isinstance(first_device_value, list) and len(first_device_value) > 0:
                first_dev = first_device_value[0]
                if isinstance(first_dev, int):
                    input_device = torch.device(f"cuda:{first_dev}")
                elif isinstance(first_dev, str):
                    input_device = torch.device(first_dev)
        
        model_inputs["images"] = model_inputs["images"].to(input_device, dtype=dtype)
        model_inputs["images_spatial_crop"] = model_inputs["images_spatial_crop"].to(input_device)
        model_inputs["images_seq_mask"] = model_inputs["images_seq_mask"].to(input_device)
        model_inputs["input_ids"] = model_inputs["input_ids"].to(input_device)
        model_inputs["attention_mask"] = model_inputs["attention_mask"].to(input_device)
        
        # Clear cache trÆ°á»›c khi prepare inputs
        if self.low_memory_mode:
            self._clear_cuda_cache()
        
        # Generate response
        try:
            with torch.no_grad():
                inputs_embeds = self.model.prepare_inputs_embeds(**model_inputs)
                
                if self.low_memory_mode:
                    self._clear_cuda_cache()
                
                attention_mask = model_inputs["attention_mask"]
                
                outputs = self.model.language.generate(
                    inputs_embeds=inputs_embeds,
                    attention_mask=attention_mask,
                    max_new_tokens=self.max_new_tokens,
                    pad_token_id=self.tokenizer.eos_token_id,
                    eos_token_id=self.tokenizer.eos_token_id,
                    do_sample=False,
                    use_cache=True,
                )
            
            # Decode response
            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            # Clear memory sau khi generate
            del inputs_embeds
            del outputs
            del attention_mask
            if self.low_memory_mode:
                self._clear_cuda_cache()
                
        except RuntimeError as e:
            if "out of memory" in str(e).lower():
                print(f"âŒ CUDA out of memory error occurred!")
                print(f"ğŸ’¡ Suggestions:")
                print(f"   1. Use low_memory_mode=True when initializing detector")
                print(f"   2. Set max_image_size to a smaller value (e.g., 1024)")
                print(f"   3. Process smaller images or reduce max_new_tokens")
                if self.device.type == "cuda":
                    self._clear_cuda_cache()
                raise
            else:
                raise
        
        print("\n========== MODEL RESPONSE ==========\n")
        print(response)
        print("\n====================================\n")
        
        # Parse response
        result = parse_response(response, img_width=img_width, img_height=img_height)
        
        # Set width/height náº¿u chÆ°a cÃ³
        if result.width == 0:
            result.width = img_width
        if result.height == 0:
            result.height = img_height
        
        # Validate vÃ  normalize coordinates
        for bbox in result.get_all():
            bbox.x1 = max(0, min(bbox.x1, img_width - 1))
            bbox.y1 = max(0, min(bbox.y1, img_height - 1))
            bbox.x2 = max(bbox.x1 + 1, min(bbox.x2, img_width))
            bbox.y2 = max(bbox.y1 + 1, min(bbox.y2, img_height))
        
        if return_image:
            annotated_image = draw_boxes(image.copy(), result)
            return result, annotated_image
        
        return result
    
    def detect_batch(
        self,
        image_paths: List[str],
        language: str = "vietnamese",
        return_images: bool = False,
    ) -> List[Tuple[DetectionResult, Optional[Image.Image]]]:
        """
        Nháº­n diá»‡n con dáº¥u vÃ  chá»¯ kÃ½ cho nhiá»u áº£nh
        
        Args:
            image_paths: Danh sÃ¡ch Ä‘Æ°á»ng dáº«n Ä‘áº¿n cÃ¡c file áº£nh
            language: NgÃ´n ngá»¯ cá»§a prompt
            return_images: Náº¿u True, tráº£ vá» thÃªm cÃ¡c PIL Images Ä‘Ã£ Ä‘Æ°á»£c váº½ bounding boxes
        
        Returns:
            List of (DetectionResult, Optional[PIL.Image]) tuples
        """
        results = []
        for i, image_path in enumerate(image_paths, 1):
            print(f"\n[{i}/{len(image_paths)}] Processing: {image_path}")
            try:
                if return_images:
                    result, annotated_image = self.detect(
                        image_path=image_path,
                        language=language,
                        return_image=True
                    )
                    results.append((result, annotated_image))
                else:
                    result = self.detect(
                        image_path=image_path,
                        language=language,
                        return_image=False
                    )
                    results.append((result, None))
            except Exception as e:
                print(f"âŒ Error processing {image_path}: {e}")
                # Táº¡o empty result Ä‘á»ƒ giá»¯ index
                empty_result = DetectionResult()
                results.append((empty_result, None))
        
        return results
    
    def save_result_image(
        self,
        image_path: str,
        result: DetectionResult,
        output_path: str,
        seal_color: str = "red",
        signature_color: str = "blue",
    ):
        """
        LÆ°u áº£nh Ä‘Ã£ Ä‘Æ°á»£c váº½ bounding boxes
        
        Args:
            image_path: ÄÆ°á»ng dáº«n Ä‘áº¿n áº£nh gá»‘c
            result: DetectionResult
            output_path: ÄÆ°á»ng dáº«n Ä‘á»ƒ lÆ°u áº£nh káº¿t quáº£
            seal_color: MÃ u Ä‘á»ƒ váº½ con dáº¥u
            signature_color: MÃ u Ä‘á»ƒ váº½ chá»¯ kÃ½
        """
        image = Image.open(image_path).convert("RGB")
        annotated_image = draw_boxes(image, result, seal_color, signature_color)
        annotated_image.save(output_path)
        print(f"âœ… Saved annotated image to {output_path}")
    
    def release_memory(self):
        """Giáº£i phÃ³ng model khá»i GPU"""
        self.model_manager.release_memory()

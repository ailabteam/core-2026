# ==============================
# DeepSeek-VL2 Seal & Signature Detector
# ==============================
"""
Module Ä‘á»ƒ nháº­n diá»‡n vÃ  Ä‘á»‹nh vá»‹ con dáº¥u (seal/stamp) vÃ  chá»¯ kÃ½ (signature) 
trong hÃ¬nh áº£nh tÃ i liá»‡u sá»­ dá»¥ng DeepSeek-VL2.

File nÃ y giá»¯ láº¡i Ä‘á»ƒ backward compatibility.
Sá»­ dá»¥ng tá»« detector.py vÃ  cÃ¡c module khÃ¡c Ä‘á»ƒ tá»‘i Æ°u memory vÃ  reuse model.
"""

import os
import json
import torch
from typing import List, Optional

# Import tá»« cÃ¡c module má»›i
from .detector import SealSignatureDetector
from .models import BoundingBox, DetectionResult
from .model_manager import ModelManager

# Export Ä‘á»ƒ backward compatibility
__all__ = ["SealSignatureDetector", "BoundingBox", "DetectionResult", "ModelManager"]


# ------------------------------
# MAIN / Example Usage
# ------------------------------
def main():
    """
    Example usage - Single image detection
    
    Model sáº½ chá»‰ Ä‘Æ°á»£c load má»™t láº§n vÃ  reuse cho cÃ¡c láº§n cháº¡y tiáº¿p theo
    """
    # Setup
    model_name = "deepseek-ai/deepseek-vl2-tiny"
    image_path = "/kaggle/input/test-sign/1-5_Opt.jpg"  # Äiá»u chá»‰nh Ä‘Æ°á»ng dáº«n theo cáº§n thiáº¿t
    
    # Kiá»ƒm tra file áº£nh tá»“n táº¡i
    if not os.path.exists(image_path):
        print(f"âŒ Image not found: {image_path}")
        print("Please update the image_path variable")
        return
    
    # Khá»Ÿi táº¡o detector vá»›i multi-GPU vÃ  low_memory_mode
    # Model sáº½ chá»‰ Ä‘Æ°á»£c load má»™t láº§n (singleton pattern)
    num_gpus = torch.cuda.device_count() if torch.cuda.is_available() else 0
    max_img_size = 1536 if num_gpus > 1 else 1024
    
    detector = SealSignatureDetector(
        model_path=model_name,
        low_memory_mode=True,
        max_image_size=max_img_size,
        use_multi_gpu=True,
        device_map="auto",
    )
    
    # Nháº­n diá»‡n
    print(f"\nðŸ” Detecting seals and signatures in: {image_path}\n")
    result, annotated_image = detector.detect(
        image_path=image_path,
        language="vietnamese",
        return_image=True
    )
    
    # In káº¿t quáº£
    print(f"\nðŸ“Š Detection Results:")
    print(f"  - Seals found: {len(result.seals)}")
    print(f"  - Signatures found: {len(result.signatures)}")
    
    if result.seals:
        print("\n  ðŸ·ï¸  Seals:")
        for i, seal in enumerate(result.seals, 1):
            print(f"    {i}. {seal}")
    
    if result.signatures:
        print("\n  âœï¸  Signatures:")
        for i, sig in enumerate(result.signatures, 1):
            print(f"    {i}. {sig}")
    
    # LÆ°u káº¿t quáº£
    base_name = os.path.basename(image_path)
    name_without_ext = os.path.splitext(base_name)[0]
    
    # XÃ¡c Ä‘á»‹nh output directory
    if os.path.exists("/kaggle/working"):
        output_dir = "/kaggle/working"
    else:
        output_dir = os.path.dirname(image_path) if os.path.dirname(image_path) else "."
    
    output_json_path = os.path.join(output_dir, f"{name_without_ext}_detection.json")
    with open(output_json_path, "w", encoding="utf-8") as f:
        json.dump(result.to_json_format(), f, indent=2, ensure_ascii=False)
    print(f"\nâœ… Saved JSON results to: {output_json_path}")
    
    # LÆ°u áº£nh Ä‘Ã£ Ä‘Æ°á»£c váº½
    ext = os.path.splitext(base_name)[1] or ".jpg"
    output_image_path = os.path.join(output_dir, f"{name_without_ext}_detected{ext}")
    annotated_image.save(output_image_path)
    print(f"âœ… Saved annotated image to: {output_image_path}")


def main_batch():
    """
    Example usage - Batch processing nhiá»u áº£nh
    
    Model chá»‰ Ä‘Æ°á»£c load má»™t láº§n vÃ  reuse cho táº¥t cáº£ cÃ¡c áº£nh
    """
    # Setup
    model_name = "deepseek-ai/deepseek-vl2-tiny"
    image_paths = [
        "/kaggle/input/test-sign/image1.jpg",
        "/kaggle/input/test-sign/image2.jpg",
        "/kaggle/input/test-sign/image3.jpg",
    ]
    
    # Filter chá»‰ cÃ¡c file tá»“n táº¡i
    image_paths = [p for p in image_paths if os.path.exists(p)]
    
    if not image_paths:
        print("âŒ No images found")
        return
    
    # Khá»Ÿi táº¡o detector má»™t láº§n
    # Model sáº½ Ä‘Æ°á»£c load vÃ  reuse cho táº¥t cáº£ cÃ¡c áº£nh
    num_gpus = torch.cuda.device_count() if torch.cuda.is_available() else 0
    max_img_size = 1536 if num_gpus > 1 else 1024
    
    print(f"ðŸš€ Initializing detector (model will be loaded once)...")
    detector = SealSignatureDetector(
        model_path=model_name,
        low_memory_mode=True,
        max_image_size=max_img_size,
        use_multi_gpu=True,
        device_map="auto",
    )
    
    print(f"\nðŸ“¦ Processing {len(image_paths)} images...")
    print("   (Model is reused for all images - no reloading)\n")
    
    # Batch processing
    results = detector.detect_batch(
        image_paths=image_paths,
        language="vietnamese",
        return_images=True
    )
    
    # LÆ°u káº¿t quáº£ cho má»—i áº£nh
    if os.path.exists("/kaggle/working"):
        output_dir = "/kaggle/working"
    else:
        output_dir = "."
    
    for (result, annotated_image), image_path in zip(results, image_paths):
        base_name = os.path.basename(image_path)
        name_without_ext = os.path.splitext(base_name)[0]
        
        # LÆ°u JSON
        output_json_path = os.path.join(output_dir, f"{name_without_ext}_detection.json")
        with open(output_json_path, "w", encoding="utf-8") as f:
            json.dump(result.to_json_format(), f, indent=2, ensure_ascii=False)
        
        # LÆ°u áº£nh
        if annotated_image:
            ext = os.path.splitext(base_name)[1] or ".jpg"
            output_image_path = os.path.join(output_dir, f"{name_without_ext}_detected{ext}")
            annotated_image.save(output_image_path)
        
        print(f"âœ… Processed: {base_name} - {len(result.seals)} seals, {len(result.signatures)} signatures")
    
    print(f"\nâœ… All {len(image_paths)} images processed!")
    print("ðŸ’¡ Model was loaded only once and reused for all images")


if __name__ == "__main__":
    # Cháº¡y single image detection
    main()
    
    # Hoáº·c cháº¡y batch processing
    # main_batch()

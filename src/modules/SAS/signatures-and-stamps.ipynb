{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "execution": {
          "iopub.execute_input": "2025-12-18T16:53:48.799049Z",
          "iopub.status.busy": "2025-12-18T16:53:48.798514Z",
          "iopub.status.idle": "2025-12-18T16:54:05.631207Z",
          "shell.execute_reply": "2025-12-18T16:54:05.630497Z",
          "shell.execute_reply.started": "2025-12-18T16:53:48.799015Z"
        },
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.11/dist-packages (24.1.2)\n",
            "Collecting pip\n",
            "  Downloading pip-25.3-py3-none-any.whl.metadata (4.7 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (75.2.0)\n",
            "Collecting setuptools\n",
            "  Downloading setuptools-80.9.0-py3-none-any.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.11/dist-packages (0.45.1)\n",
            "Downloading pip-25.3-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading setuptools-80.9.0-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m64.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: setuptools, pip\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 75.2.0\n",
            "    Uninstalling setuptools-75.2.0:\n",
            "      Successfully uninstalled setuptools-75.2.0\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 24.1.2\n",
            "    Uninstalling pip-24.1.2:\n",
            "      Successfully uninstalled pip-24.1.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "tensorflow 2.18.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3, but you have protobuf 6.33.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed pip-25.3 setuptools-80.9.0\n"
          ]
        }
      ],
      "source": [
        "!pip install -U pip setuptools wheel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-12-18T16:54:12.412937Z",
          "iopub.status.busy": "2025-12-18T16:54:12.412268Z",
          "iopub.status.idle": "2025-12-18T16:55:58.652158Z",
          "shell.execute_reply": "2025-12-18T16:55:58.651173Z",
          "shell.execute_reply.started": "2025-12-18T16:54:12.412906Z"
        },
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
            "Collecting torch==2.0.1+cu118\n",
            "  Downloading https://download.pytorch.org/whl/cu118/torch-2.0.1%2Bcu118-cp311-cp311-linux_x86_64.whl (2267.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 GB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m  \u001b[33m0:00:32\u001b[0m6m0:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting torchvision==0.15.2+cu118\n",
            "  Downloading https://download.pytorch.org/whl/cu118/torchvision-0.15.2%2Bcu118-cp311-cp311-linux_x86_64.whl (6.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.1/6.1 MB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m6m0:00:01\u001b[0m\n",
            "\u001b[?25hCollecting torchaudio==2.0.2+cu118\n",
            "  Downloading https://download.pytorch.org/whl/cu118/torchaudio-2.0.2%2Bcu118-cp311-cp311-linux_x86_64.whl (4.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m133.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1+cu118) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1+cu118) (4.15.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1+cu118) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1+cu118) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1+cu118) (3.1.6)\n",
            "Collecting triton==2.0.0 (from torch==2.0.1+cu118)\n",
            "  Downloading https://download.pytorch.org/whl/triton-2.0.0-1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.3/63.3 MB\u001b[0m \u001b[31m138.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m0:00:01\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision==0.15.2+cu118) (1.26.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torchvision==0.15.2+cu118) (2.32.5)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision==0.15.2+cu118) (11.3.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.11/dist-packages (from triton==2.0.0->torch==2.0.1+cu118) (3.31.6)\n",
            "Collecting lit (from triton==2.0.0->torch==2.0.1+cu118)\n",
            "  Downloading https://download.pytorch.org/whl/lit-15.0.7.tar.gz (132 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.0.1+cu118) (3.0.3)\n",
            "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision==0.15.2+cu118) (1.3.8)\n",
            "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision==0.15.2+cu118) (1.2.4)\n",
            "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision==0.15.2+cu118) (0.1.1)\n",
            "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision==0.15.2+cu118) (2025.3.0)\n",
            "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision==0.15.2+cu118) (2022.3.0)\n",
            "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision==0.15.2+cu118) (2.4.1)\n",
            "Requirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision==0.15.2+cu118) (2025.3.0)\n",
            "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision==0.15.2+cu118) (2024.2.0)\n",
            "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision==0.15.2+cu118) (2022.3.0)\n",
            "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->torchvision==0.15.2+cu118) (2024.2.0)\n",
            "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->torchvision==0.15.2+cu118) (1.4.0)\n",
            "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->torchvision==0.15.2+cu118) (2024.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision==0.15.2+cu118) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision==0.15.2+cu118) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision==0.15.2+cu118) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision==0.15.2+cu118) (2025.10.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.0.1+cu118) (1.3.0)\n",
            "Building wheels for collected packages: lit\n",
            "  Building wheel for lit (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lit: filename=lit-15.0.7-py3-none-any.whl size=89991 sha256=ca80debff2399e37e60cbc1e4085a5595b267659f524e6c14e348690098f0fe3\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/5d/45/34fe9945d5e45e261134e72284395be36c2d4828af38e2b0fe\n",
            "Successfully built lit\n",
            "Installing collected packages: lit, triton, torch, torchvision, torchaudio\n",
            "\u001b[2K  Attempting uninstall: triton\n",
            "\u001b[2K    Found existing installation: triton 3.2.0\n",
            "\u001b[2K    Uninstalling triton-3.2.0:\n",
            "\u001b[2K      Successfully uninstalled triton-3.2.0━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/5\u001b[0m [triton]\n",
            "\u001b[2K  Attempting uninstall: torchm\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/5\u001b[0m [triton]\n",
            "\u001b[2K    Found existing installation: torch 2.6.0+cu124━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/5\u001b[0m [torch]\n",
            "\u001b[2K    Uninstalling torch-2.6.0+cu124:[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/5\u001b[0m [torch]\n",
            "\u001b[2K      Successfully uninstalled torch-2.6.0+cu124━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/5\u001b[0m [torch]\n",
            "\u001b[2K  Attempting uninstall: torchvision[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/5\u001b[0m [torch]\n",
            "\u001b[2K    Found existing installation: torchvision 0.21.0+cu124━━━━━\u001b[0m \u001b[32m2/5\u001b[0m [torch]\n",
            "\u001b[2K    Uninstalling torchvision-0.21.0+cu124:━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/5\u001b[0m [torch]\n",
            "\u001b[2K      Successfully uninstalled torchvision-0.21.0+cu124━━━━━━━━━━━\u001b[0m \u001b[32m3/5\u001b[0m [torchvision]\n",
            "\u001b[2K  Attempting uninstall: torchaudio0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/5\u001b[0m [torchvision]\n",
            "\u001b[2K    Found existing installation: torchaudio 2.6.0+cu124━━━━━━━\u001b[0m \u001b[32m3/5\u001b[0m [torchvision]\n",
            "\u001b[2K    Uninstalling torchaudio-2.6.0+cu124:0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/5\u001b[0m [torchvision]\n",
            "\u001b[2K      Successfully uninstalled torchaudio-2.6.0+cu124m\u001b[90m━━━━━━━\u001b[0m \u001b[32m4/5\u001b[0m [torchaudio]\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5/5\u001b[0m [torchaudio]5\u001b[0m [torchaudio]\n",
            "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "pytorch-lightning 2.5.5 requires torch>=2.1.0, but you have torch 2.0.1+cu118 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed lit-15.0.7 torch-2.0.1+cu118 torchaudio-2.0.2+cu118 torchvision-0.15.2+cu118 triton-2.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install torch==2.0.1+cu118 torchvision==0.15.2+cu118 torchaudio==2.0.2+cu118 \\\n",
        "  --index-url https://download.pytorch.org/whl/cu118"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-12-18T17:01:13.611735Z",
          "iopub.status.busy": "2025-12-18T17:01:13.610907Z",
          "iopub.status.idle": "2025-12-18T17:01:21.350925Z",
          "shell.execute_reply": "2025-12-18T17:01:21.350232Z",
          "shell.execute_reply.started": "2025-12-18T17:01:13.611698Z"
        },
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting xformers==0.0.21\n",
            "  Downloading xformers-0.0.21-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.0 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from xformers==0.0.21) (1.26.4)\n",
            "Requirement already satisfied: torch==2.0.1 in /usr/local/lib/python3.11/dist-packages (from xformers==0.0.21) (2.0.1+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1->xformers==0.0.21) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1->xformers==0.0.21) (4.15.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1->xformers==0.0.21) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1->xformers==0.0.21) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1->xformers==0.0.21) (3.1.6)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1->xformers==0.0.21) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.11/dist-packages (from triton==2.0.0->torch==2.0.1->xformers==0.0.21) (3.31.6)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.11/dist-packages (from triton==2.0.0->torch==2.0.1->xformers==0.0.21) (15.0.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.0.1->xformers==0.0.21) (3.0.3)\n",
            "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->xformers==0.0.21) (1.3.8)\n",
            "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->xformers==0.0.21) (1.2.4)\n",
            "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->xformers==0.0.21) (0.1.1)\n",
            "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->xformers==0.0.21) (2025.3.0)\n",
            "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->xformers==0.0.21) (2022.3.0)\n",
            "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->xformers==0.0.21) (2.4.1)\n",
            "Requirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->xformers==0.0.21) (2025.3.0)\n",
            "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->xformers==0.0.21) (2024.2.0)\n",
            "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->xformers==0.0.21) (2022.3.0)\n",
            "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->xformers==0.0.21) (2024.2.0)\n",
            "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->xformers==0.0.21) (1.4.0)\n",
            "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->xformers==0.0.21) (2024.2.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.0.1->xformers==0.0.21) (1.3.0)\n",
            "Downloading xformers-0.0.21-cp311-cp311-manylinux2014_x86_64.whl (167.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.0/167.0 MB\u001b[0m \u001b[31m69.9 MB/s\u001b[0m  \u001b[33m0:00:02\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xformers\n",
            "Successfully installed xformers-0.0.21\n"
          ]
        }
      ],
      "source": [
        "!pip install xformers==0.0.21"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-12-18T17:01:33.370817Z",
          "iopub.status.busy": "2025-12-18T17:01:33.370148Z",
          "iopub.status.idle": "2025-12-18T17:02:02.271703Z",
          "shell.execute_reply": "2025-12-18T17:02:02.270751Z",
          "shell.execute_reply.started": "2025-12-18T17:01:33.370784Z"
        },
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/huggingface/transformers\n",
            "  Cloning https://github.com/huggingface/transformers to /tmp/pip-req-build-1ya7fug1\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers /tmp/pip-req-build-1ya7fug1\n",
            "  Resolved https://github.com/huggingface/transformers to commit af91c0ba849b471a4967931092a187ac0b1125e1\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==5.0.0.dev0) (3.20.0)\n",
            "Collecting huggingface-hub<2.0,>=1.2.1 (from transformers==5.0.0.dev0)\n",
            "  Downloading huggingface_hub-1.2.3-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==5.0.0.dev0) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==5.0.0.dev0) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==5.0.0.dev0) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==5.0.0.dev0) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==5.0.0.dev0) (2.32.5)\n",
            "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers==5.0.0.dev0)\n",
            "  Downloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: typer-slim in /usr/local/lib/python3.11/dist-packages (from transformers==5.0.0.dev0) (0.20.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers==5.0.0.dev0) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==5.0.0.dev0) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<2.0,>=1.2.1->transformers==5.0.0.dev0) (2025.10.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<2.0,>=1.2.1->transformers==5.0.0.dev0) (1.2.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<2.0,>=1.2.1->transformers==5.0.0.dev0) (0.28.1)\n",
            "Requirement already satisfied: shellingham in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<2.0,>=1.2.1->transformers==5.0.0.dev0) (1.5.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<2.0,>=1.2.1->transformers==5.0.0.dev0) (4.15.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.2.1->transformers==5.0.0.dev0) (4.11.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.2.1->transformers==5.0.0.dev0) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.2.1->transformers==5.0.0.dev0) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.2.1->transformers==5.0.0.dev0) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.2.1->transformers==5.0.0.dev0) (0.16.0)\n",
            "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==5.0.0.dev0) (1.3.8)\n",
            "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==5.0.0.dev0) (1.2.4)\n",
            "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==5.0.0.dev0) (0.1.1)\n",
            "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==5.0.0.dev0) (2025.3.0)\n",
            "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==5.0.0.dev0) (2022.3.0)\n",
            "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==5.0.0.dev0) (2.4.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.2.1->transformers==5.0.0.dev0) (1.3.1)\n",
            "Requirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers==5.0.0.dev0) (2025.3.0)\n",
            "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers==5.0.0.dev0) (2024.2.0)\n",
            "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers==5.0.0.dev0) (2022.3.0)\n",
            "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers==5.0.0.dev0) (2024.2.0)\n",
            "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers==5.0.0.dev0) (1.4.0)\n",
            "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers==5.0.0.dev0) (2024.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==5.0.0.dev0) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==5.0.0.dev0) (2.5.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer-slim->transformers==5.0.0.dev0) (8.3.0)\n",
            "Downloading huggingface_hub-1.2.3-py3-none-any.whl (520 kB)\n",
            "Downloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m68.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: transformers\n",
            "  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-5.0.0.dev0-py3-none-any.whl size=11008538 sha256=c209907d0d23e51977b3fc2284748a059c3cfaaf70b6b54114335f94a0f56b0c\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-etkczibt/wheels/04/a3/f1/b88775f8e1665827525b19ac7590250f1038d947067beba9fb\n",
            "Successfully built transformers\n",
            "Installing collected packages: huggingface-hub, tokenizers, transformers\n",
            "\u001b[2K  Attempting uninstall: huggingface-hub\n",
            "\u001b[2K    Found existing installation: huggingface-hub 0.36.0\n",
            "\u001b[2K    Uninstalling huggingface-hub-0.36.0:\n",
            "\u001b[2K      Successfully uninstalled huggingface-hub-0.36.0\u001b[32m0/3\u001b[0m [huggingface-hub]\n",
            "\u001b[2K  Attempting uninstall: tokenizers━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/3\u001b[0m [huggingface-hub]\n",
            "\u001b[2K    Found existing installation: tokenizers 0.21.2[32m0/3\u001b[0m [huggingface-hub]\n",
            "\u001b[2K    Uninstalling tokenizers-0.21.2:━━━━━━━━━\u001b[0m \u001b[32m0/3\u001b[0m [huggingface-hub]\n",
            "\u001b[2K      Successfully uninstalled tokenizers-0.21.2━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/3\u001b[0m [tokenizers]\n",
            "\u001b[2K  Attempting uninstall: transformers━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/3\u001b[0m [tokenizers]\n",
            "\u001b[2K    Found existing installation: transformers 4.53.3━━━━━━━━━━\u001b[0m \u001b[32m1/3\u001b[0m [tokenizers]\n",
            "\u001b[2K    Uninstalling transformers-4.53.3:m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m2/3\u001b[0m [transformers]\n",
            "\u001b[2K      Successfully uninstalled transformers-4.53.390m━━━━━━━━━━━━━\u001b[0m \u001b[32m2/3\u001b[0m [transformers]\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/3\u001b[0m [transformers][0m [transformers]\n",
            "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datasets 4.4.1 requires pyarrow>=21.0.0, but you have pyarrow 19.0.1 which is incompatible.\n",
            "sentence-transformers 4.1.0 requires transformers<5.0.0,>=4.41.0, but you have transformers 5.0.0.dev0 which is incompatible.\n",
            "gradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed huggingface-hub-1.2.3 tokenizers-0.22.1 transformers-5.0.0.dev0\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/huggingface/transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-12-18T17:03:00.178146Z",
          "iopub.status.busy": "2025-12-18T17:03:00.177345Z",
          "iopub.status.idle": "2025-12-18T17:03:02.591447Z",
          "shell.execute_reply": "2025-12-18T17:03:02.590606Z",
          "shell.execute_reply.started": "2025-12-18T17:03:00.178109Z"
        },
        "trusted": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Cloning into 'DeepSeek-VL2'...\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "git clone https://github.com/deepseek-ai/DeepSeek-VL2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-12-18T17:03:13.521374Z",
          "iopub.status.busy": "2025-12-18T17:03:13.520626Z",
          "iopub.status.idle": "2025-12-18T17:03:13.526312Z",
          "shell.execute_reply": "2025-12-18T17:03:13.525689Z",
          "shell.execute_reply.started": "2025-12-18T17:03:13.521346Z"
        },
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/kaggle/working/DeepSeek-VL2\n"
          ]
        }
      ],
      "source": [
        "%cd DeepSeek-VL2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-12-18T17:03:16.443883Z",
          "iopub.status.busy": "2025-12-18T17:03:16.443279Z",
          "iopub.status.idle": "2025-12-18T17:03:30.037016Z",
          "shell.execute_reply": "2025-12-18T17:03:30.036338Z",
          "shell.execute_reply.started": "2025-12-18T17:03:16.443859Z"
        },
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Obtaining file:///kaggle/working/DeepSeek-VL2\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch==2.0.1 in /usr/local/lib/python3.11/dist-packages (from deepseek_vl2==1.0.0) (2.0.1+cu118)\n",
            "Collecting transformers==4.38.2 (from deepseek_vl2==1.0.0)\n",
            "  Downloading transformers-4.38.2-py3-none-any.whl.metadata (130 kB)\n",
            "Requirement already satisfied: timm>=0.9.16 in /usr/local/lib/python3.11/dist-packages (from deepseek_vl2==1.0.0) (1.0.19)\n",
            "Requirement already satisfied: xformers>=0.0.21 in /usr/local/lib/python3.11/dist-packages (from deepseek_vl2==1.0.0) (0.0.21)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (from deepseek_vl2==1.0.0) (1.9.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from deepseek_vl2==1.0.0) (0.2.0)\n",
            "Collecting attrdict (from deepseek_vl2==1.0.0)\n",
            "  Downloading attrdict-2.0.1-py2.py3-none-any.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from deepseek_vl2==1.0.0) (0.8.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1->deepseek_vl2==1.0.0) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1->deepseek_vl2==1.0.0) (4.15.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1->deepseek_vl2==1.0.0) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1->deepseek_vl2==1.0.0) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1->deepseek_vl2==1.0.0) (3.1.6)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1->deepseek_vl2==1.0.0) (2.0.0)\n",
            "Collecting huggingface-hub<1.0,>=0.19.3 (from transformers==4.38.2->deepseek_vl2==1.0.0)\n",
            "  Downloading huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.38.2->deepseek_vl2==1.0.0) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.38.2->deepseek_vl2==1.0.0) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.38.2->deepseek_vl2==1.0.0) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.38.2->deepseek_vl2==1.0.0) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.38.2->deepseek_vl2==1.0.0) (2.32.5)\n",
            "Collecting tokenizers<0.19,>=0.14 (from transformers==4.38.2->deepseek_vl2==1.0.0)\n",
            "  Downloading tokenizers-0.15.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.38.2->deepseek_vl2==1.0.0) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.38.2->deepseek_vl2==1.0.0) (4.67.1)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.11/dist-packages (from triton==2.0.0->torch==2.0.1->deepseek_vl2==1.0.0) (3.31.6)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.11/dist-packages (from triton==2.0.0->torch==2.0.1->deepseek_vl2==1.0.0) (15.0.7)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.38.2->deepseek_vl2==1.0.0) (2025.10.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.38.2->deepseek_vl2==1.0.0) (1.2.0)\n",
            "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.38.2->deepseek_vl2==1.0.0) (1.3.8)\n",
            "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.38.2->deepseek_vl2==1.0.0) (1.2.4)\n",
            "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.38.2->deepseek_vl2==1.0.0) (0.1.1)\n",
            "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.38.2->deepseek_vl2==1.0.0) (2025.3.0)\n",
            "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.38.2->deepseek_vl2==1.0.0) (2022.3.0)\n",
            "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.38.2->deepseek_vl2==1.0.0) (2.4.1)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from timm>=0.9.16->deepseek_vl2==1.0.0) (0.15.2+cu118)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate->deepseek_vl2==1.0.0) (7.1.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from attrdict->deepseek_vl2==1.0.0) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.0.1->deepseek_vl2==1.0.0) (3.0.3)\n",
            "Requirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers==4.38.2->deepseek_vl2==1.0.0) (2025.3.0)\n",
            "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers==4.38.2->deepseek_vl2==1.0.0) (2024.2.0)\n",
            "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers==4.38.2->deepseek_vl2==1.0.0) (2022.3.0)\n",
            "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers==4.38.2->deepseek_vl2==1.0.0) (2024.2.0)\n",
            "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers==4.38.2->deepseek_vl2==1.0.0) (1.4.0)\n",
            "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers==4.38.2->deepseek_vl2==1.0.0) (2024.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.38.2->deepseek_vl2==1.0.0) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.38.2->deepseek_vl2==1.0.0) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.38.2->deepseek_vl2==1.0.0) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.38.2->deepseek_vl2==1.0.0) (2025.10.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.0.1->deepseek_vl2==1.0.0) (1.3.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision->timm>=0.9.16->deepseek_vl2==1.0.0) (11.3.0)\n",
            "Downloading transformers-4.38.2-py3-none-any.whl (8.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.5/8.5 MB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m566.1/566.1 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.15.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m38.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading attrdict-2.0.1-py2.py3-none-any.whl (9.9 kB)\n",
            "Building wheels for collected packages: deepseek_vl2\n",
            "  Building editable for deepseek_vl2 (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for deepseek_vl2: filename=deepseek_vl2-1.0.0-0.editable-py3-none-any.whl size=14553 sha256=be765d869468ec2ccb44458b1a4eaff92ab52ee9749989e09184fa6e5564e4dc\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-51v1ay09/wheels/c7/0a/ec/dd262d2d48e0c2858d56d6aa53ad9be1fa6a602d732bced2ea\n",
            "Successfully built deepseek_vl2\n",
            "Installing collected packages: attrdict, huggingface-hub, tokenizers, transformers, deepseek_vl2\n",
            "\u001b[2K  Attempting uninstall: huggingface-hub\n",
            "\u001b[2K    Found existing installation: huggingface_hub 1.2.3\n",
            "\u001b[2K    Uninstalling huggingface_hub-1.2.3:\n",
            "\u001b[2K      Successfully uninstalled huggingface_hub-1.2.3\n",
            "\u001b[2K  Attempting uninstall: tokenizersm━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/5\u001b[0m [huggingface-hub]\n",
            "\u001b[2K    Found existing installation: tokenizers 0.22.1━━━━━━━━━━━━\u001b[0m \u001b[32m1/5\u001b[0m [huggingface-hub]\n",
            "\u001b[2K    Uninstalling tokenizers-0.22.1:━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/5\u001b[0m [huggingface-hub]\n",
            "\u001b[2K      Successfully uninstalled tokenizers-0.22.1━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/5\u001b[0m [huggingface-hub]\n",
            "\u001b[2K  Attempting uninstall: transformers━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/5\u001b[0m [huggingface-hub]\n",
            "\u001b[2K    Found existing installation: transformers 5.0.0.dev0━━━━━━\u001b[0m \u001b[32m1/5\u001b[0m [huggingface-hub]\n",
            "\u001b[2K    Uninstalling transformers-5.0.0.dev0:╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/5\u001b[0m [transformers]\n",
            "\u001b[2K      Successfully uninstalled transformers-5.0.0.dev0━━━━━━━━\u001b[0m \u001b[32m3/5\u001b[0m [transformers]\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5/5\u001b[0m [deepseek_vl2][0m [transformers]\n",
            "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datasets 4.4.1 requires pyarrow>=21.0.0, but you have pyarrow 19.0.1 which is incompatible.\n",
            "sentence-transformers 4.1.0 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.38.2 which is incompatible.\n",
            "gradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed attrdict-2.0.1 deepseek_vl2-1.0.0 huggingface-hub-0.36.0 tokenizers-0.15.2 transformers-4.38.2\n"
          ]
        }
      ],
      "source": [
        "!pip install -e ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-12-18T17:03:57.362957Z",
          "iopub.status.busy": "2025-12-18T17:03:57.362652Z",
          "iopub.status.idle": "2025-12-18T17:04:06.161772Z",
          "shell.execute_reply": "2025-12-18T17:04:06.161010Z",
          "shell.execute_reply.started": "2025-12-18T17:03:57.362926Z"
        },
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Python version is above 3.10, patching the collections module.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OK\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import xformers\n",
        "import transformers\n",
        "import torch\n",
        "from deepseek_vl2.models import DeepseekVLV2ForCausalLM\n",
        "from deepseek_vl2.models import DeepseekVLV2Processor\n",
        "\n",
        "print(\"OK\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-12-18T17:04:09.003837Z",
          "iopub.status.busy": "2025-12-18T17:04:09.003559Z",
          "iopub.status.idle": "2025-12-18T17:04:44.684503Z",
          "shell.execute_reply": "2025-12-18T17:04:44.683758Z",
          "shell.execute_reply.started": "2025-12-18T17:04:09.003817Z"
        },
        "trusted": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c87e909909df414dab2d5319042a7778",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b723104c49ae4e09b9b301c15389e826",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3fd715bafcd64248b0830b549224e27e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/801 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b9e0d43834314e6cb8b93776c3e2c671",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "processor_config.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Add pad token = ['<｜▁pad▁｜>'] to the tokenizer\n",
            "<｜▁pad▁｜>:2\n",
            "Add image token = ['<image>'] to the tokenizer\n",
            "<image>:128815\n",
            "Add grounding-related tokens = ['<|ref|>', '<|/ref|>', '<|det|>', '<|/det|>', '<|grounding|>'] to the tokenizer with input_ids\n",
            "<|ref|>:128816\n",
            "<|/ref|>:128817\n",
            "<|det|>:128818\n",
            "<|/det|>:128819\n",
            "<|grounding|>:128820\n",
            "Add chat tokens = ['<|User|>', '<|Assistant|>'] to the tokenizer with input_ids\n",
            "<|User|>:128821\n",
            "<|Assistant|>:128822\n",
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "593af5d1826149ddbc62dcd37a3d100f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a9cd0c41409749309a276138f0746638",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f50865e2faf0456aac6330848380e9cc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading shards:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "00723621385e41639131d564839755b8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00001-of-000001.safetensors:   0%|          | 0.00/6.74G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/accelerate/utils/modeling.py:1614: UserWarning: The following device_map keys do not match any submodules in the model: ['image_newline', 'view_seperator']\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "DeepseekVLV2ForCausalLM(\n",
              "  (vision): VisionTransformer(\n",
              "    (patch_embed): PatchEmbed(\n",
              "      (proj): Conv2d(3, 1152, kernel_size=(14, 14), stride=(14, 14))\n",
              "      (norm): Identity()\n",
              "    )\n",
              "    (pos_drop): Dropout(p=0.0, inplace=False)\n",
              "    (patch_drop): Identity()\n",
              "    (norm_pre): Identity()\n",
              "    (blocks): Sequential(\n",
              "      (0): Block(\n",
              "        (norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (qkv): Linear(in_features=1152, out_features=3456, bias=True)\n",
              "          (q_norm): Identity()\n",
              "          (k_norm): Identity()\n",
              "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "          (proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
              "          (proj_drop): Identity()\n",
              "        )\n",
              "        (ls1): Identity()\n",
              "        (drop_path1): Identity()\n",
              "        (norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): Mlp(\n",
              "          (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
              "          (act): GELU(approximate='tanh')\n",
              "          (drop1): Dropout(p=0.0, inplace=False)\n",
              "          (norm): Identity()\n",
              "          (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
              "          (drop2): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ls2): Identity()\n",
              "        (drop_path2): Identity()\n",
              "      )\n",
              "      (1): Block(\n",
              "        (norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (qkv): Linear(in_features=1152, out_features=3456, bias=True)\n",
              "          (q_norm): Identity()\n",
              "          (k_norm): Identity()\n",
              "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "          (proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
              "          (proj_drop): Identity()\n",
              "        )\n",
              "        (ls1): Identity()\n",
              "        (drop_path1): Identity()\n",
              "        (norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): Mlp(\n",
              "          (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
              "          (act): GELU(approximate='tanh')\n",
              "          (drop1): Dropout(p=0.0, inplace=False)\n",
              "          (norm): Identity()\n",
              "          (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
              "          (drop2): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ls2): Identity()\n",
              "        (drop_path2): Identity()\n",
              "      )\n",
              "      (2): Block(\n",
              "        (norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (qkv): Linear(in_features=1152, out_features=3456, bias=True)\n",
              "          (q_norm): Identity()\n",
              "          (k_norm): Identity()\n",
              "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "          (proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
              "          (proj_drop): Identity()\n",
              "        )\n",
              "        (ls1): Identity()\n",
              "        (drop_path1): Identity()\n",
              "        (norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): Mlp(\n",
              "          (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
              "          (act): GELU(approximate='tanh')\n",
              "          (drop1): Dropout(p=0.0, inplace=False)\n",
              "          (norm): Identity()\n",
              "          (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
              "          (drop2): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ls2): Identity()\n",
              "        (drop_path2): Identity()\n",
              "      )\n",
              "      (3): Block(\n",
              "        (norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (qkv): Linear(in_features=1152, out_features=3456, bias=True)\n",
              "          (q_norm): Identity()\n",
              "          (k_norm): Identity()\n",
              "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "          (proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
              "          (proj_drop): Identity()\n",
              "        )\n",
              "        (ls1): Identity()\n",
              "        (drop_path1): Identity()\n",
              "        (norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): Mlp(\n",
              "          (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
              "          (act): GELU(approximate='tanh')\n",
              "          (drop1): Dropout(p=0.0, inplace=False)\n",
              "          (norm): Identity()\n",
              "          (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
              "          (drop2): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ls2): Identity()\n",
              "        (drop_path2): Identity()\n",
              "      )\n",
              "      (4): Block(\n",
              "        (norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (qkv): Linear(in_features=1152, out_features=3456, bias=True)\n",
              "          (q_norm): Identity()\n",
              "          (k_norm): Identity()\n",
              "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "          (proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
              "          (proj_drop): Identity()\n",
              "        )\n",
              "        (ls1): Identity()\n",
              "        (drop_path1): Identity()\n",
              "        (norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): Mlp(\n",
              "          (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
              "          (act): GELU(approximate='tanh')\n",
              "          (drop1): Dropout(p=0.0, inplace=False)\n",
              "          (norm): Identity()\n",
              "          (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
              "          (drop2): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ls2): Identity()\n",
              "        (drop_path2): Identity()\n",
              "      )\n",
              "      (5): Block(\n",
              "        (norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (qkv): Linear(in_features=1152, out_features=3456, bias=True)\n",
              "          (q_norm): Identity()\n",
              "          (k_norm): Identity()\n",
              "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "          (proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
              "          (proj_drop): Identity()\n",
              "        )\n",
              "        (ls1): Identity()\n",
              "        (drop_path1): Identity()\n",
              "        (norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): Mlp(\n",
              "          (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
              "          (act): GELU(approximate='tanh')\n",
              "          (drop1): Dropout(p=0.0, inplace=False)\n",
              "          (norm): Identity()\n",
              "          (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
              "          (drop2): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ls2): Identity()\n",
              "        (drop_path2): Identity()\n",
              "      )\n",
              "      (6): Block(\n",
              "        (norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (qkv): Linear(in_features=1152, out_features=3456, bias=True)\n",
              "          (q_norm): Identity()\n",
              "          (k_norm): Identity()\n",
              "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "          (proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
              "          (proj_drop): Identity()\n",
              "        )\n",
              "        (ls1): Identity()\n",
              "        (drop_path1): Identity()\n",
              "        (norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): Mlp(\n",
              "          (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
              "          (act): GELU(approximate='tanh')\n",
              "          (drop1): Dropout(p=0.0, inplace=False)\n",
              "          (norm): Identity()\n",
              "          (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
              "          (drop2): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ls2): Identity()\n",
              "        (drop_path2): Identity()\n",
              "      )\n",
              "      (7): Block(\n",
              "        (norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (qkv): Linear(in_features=1152, out_features=3456, bias=True)\n",
              "          (q_norm): Identity()\n",
              "          (k_norm): Identity()\n",
              "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "          (proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
              "          (proj_drop): Identity()\n",
              "        )\n",
              "        (ls1): Identity()\n",
              "        (drop_path1): Identity()\n",
              "        (norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): Mlp(\n",
              "          (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
              "          (act): GELU(approximate='tanh')\n",
              "          (drop1): Dropout(p=0.0, inplace=False)\n",
              "          (norm): Identity()\n",
              "          (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
              "          (drop2): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ls2): Identity()\n",
              "        (drop_path2): Identity()\n",
              "      )\n",
              "      (8): Block(\n",
              "        (norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (qkv): Linear(in_features=1152, out_features=3456, bias=True)\n",
              "          (q_norm): Identity()\n",
              "          (k_norm): Identity()\n",
              "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "          (proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
              "          (proj_drop): Identity()\n",
              "        )\n",
              "        (ls1): Identity()\n",
              "        (drop_path1): Identity()\n",
              "        (norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): Mlp(\n",
              "          (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
              "          (act): GELU(approximate='tanh')\n",
              "          (drop1): Dropout(p=0.0, inplace=False)\n",
              "          (norm): Identity()\n",
              "          (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
              "          (drop2): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ls2): Identity()\n",
              "        (drop_path2): Identity()\n",
              "      )\n",
              "      (9): Block(\n",
              "        (norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (qkv): Linear(in_features=1152, out_features=3456, bias=True)\n",
              "          (q_norm): Identity()\n",
              "          (k_norm): Identity()\n",
              "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "          (proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
              "          (proj_drop): Identity()\n",
              "        )\n",
              "        (ls1): Identity()\n",
              "        (drop_path1): Identity()\n",
              "        (norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): Mlp(\n",
              "          (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
              "          (act): GELU(approximate='tanh')\n",
              "          (drop1): Dropout(p=0.0, inplace=False)\n",
              "          (norm): Identity()\n",
              "          (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
              "          (drop2): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ls2): Identity()\n",
              "        (drop_path2): Identity()\n",
              "      )\n",
              "      (10): Block(\n",
              "        (norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (qkv): Linear(in_features=1152, out_features=3456, bias=True)\n",
              "          (q_norm): Identity()\n",
              "          (k_norm): Identity()\n",
              "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "          (proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
              "          (proj_drop): Identity()\n",
              "        )\n",
              "        (ls1): Identity()\n",
              "        (drop_path1): Identity()\n",
              "        (norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): Mlp(\n",
              "          (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
              "          (act): GELU(approximate='tanh')\n",
              "          (drop1): Dropout(p=0.0, inplace=False)\n",
              "          (norm): Identity()\n",
              "          (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
              "          (drop2): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ls2): Identity()\n",
              "        (drop_path2): Identity()\n",
              "      )\n",
              "      (11): Block(\n",
              "        (norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (qkv): Linear(in_features=1152, out_features=3456, bias=True)\n",
              "          (q_norm): Identity()\n",
              "          (k_norm): Identity()\n",
              "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "          (proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
              "          (proj_drop): Identity()\n",
              "        )\n",
              "        (ls1): Identity()\n",
              "        (drop_path1): Identity()\n",
              "        (norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): Mlp(\n",
              "          (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
              "          (act): GELU(approximate='tanh')\n",
              "          (drop1): Dropout(p=0.0, inplace=False)\n",
              "          (norm): Identity()\n",
              "          (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
              "          (drop2): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ls2): Identity()\n",
              "        (drop_path2): Identity()\n",
              "      )\n",
              "      (12): Block(\n",
              "        (norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (qkv): Linear(in_features=1152, out_features=3456, bias=True)\n",
              "          (q_norm): Identity()\n",
              "          (k_norm): Identity()\n",
              "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "          (proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
              "          (proj_drop): Identity()\n",
              "        )\n",
              "        (ls1): Identity()\n",
              "        (drop_path1): Identity()\n",
              "        (norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): Mlp(\n",
              "          (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
              "          (act): GELU(approximate='tanh')\n",
              "          (drop1): Dropout(p=0.0, inplace=False)\n",
              "          (norm): Identity()\n",
              "          (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
              "          (drop2): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ls2): Identity()\n",
              "        (drop_path2): Identity()\n",
              "      )\n",
              "      (13): Block(\n",
              "        (norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (qkv): Linear(in_features=1152, out_features=3456, bias=True)\n",
              "          (q_norm): Identity()\n",
              "          (k_norm): Identity()\n",
              "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "          (proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
              "          (proj_drop): Identity()\n",
              "        )\n",
              "        (ls1): Identity()\n",
              "        (drop_path1): Identity()\n",
              "        (norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): Mlp(\n",
              "          (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
              "          (act): GELU(approximate='tanh')\n",
              "          (drop1): Dropout(p=0.0, inplace=False)\n",
              "          (norm): Identity()\n",
              "          (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
              "          (drop2): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ls2): Identity()\n",
              "        (drop_path2): Identity()\n",
              "      )\n",
              "      (14): Block(\n",
              "        (norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (qkv): Linear(in_features=1152, out_features=3456, bias=True)\n",
              "          (q_norm): Identity()\n",
              "          (k_norm): Identity()\n",
              "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "          (proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
              "          (proj_drop): Identity()\n",
              "        )\n",
              "        (ls1): Identity()\n",
              "        (drop_path1): Identity()\n",
              "        (norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): Mlp(\n",
              "          (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
              "          (act): GELU(approximate='tanh')\n",
              "          (drop1): Dropout(p=0.0, inplace=False)\n",
              "          (norm): Identity()\n",
              "          (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
              "          (drop2): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ls2): Identity()\n",
              "        (drop_path2): Identity()\n",
              "      )\n",
              "      (15): Block(\n",
              "        (norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (qkv): Linear(in_features=1152, out_features=3456, bias=True)\n",
              "          (q_norm): Identity()\n",
              "          (k_norm): Identity()\n",
              "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "          (proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
              "          (proj_drop): Identity()\n",
              "        )\n",
              "        (ls1): Identity()\n",
              "        (drop_path1): Identity()\n",
              "        (norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): Mlp(\n",
              "          (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
              "          (act): GELU(approximate='tanh')\n",
              "          (drop1): Dropout(p=0.0, inplace=False)\n",
              "          (norm): Identity()\n",
              "          (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
              "          (drop2): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ls2): Identity()\n",
              "        (drop_path2): Identity()\n",
              "      )\n",
              "      (16): Block(\n",
              "        (norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (qkv): Linear(in_features=1152, out_features=3456, bias=True)\n",
              "          (q_norm): Identity()\n",
              "          (k_norm): Identity()\n",
              "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "          (proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
              "          (proj_drop): Identity()\n",
              "        )\n",
              "        (ls1): Identity()\n",
              "        (drop_path1): Identity()\n",
              "        (norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): Mlp(\n",
              "          (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
              "          (act): GELU(approximate='tanh')\n",
              "          (drop1): Dropout(p=0.0, inplace=False)\n",
              "          (norm): Identity()\n",
              "          (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
              "          (drop2): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ls2): Identity()\n",
              "        (drop_path2): Identity()\n",
              "      )\n",
              "      (17): Block(\n",
              "        (norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (qkv): Linear(in_features=1152, out_features=3456, bias=True)\n",
              "          (q_norm): Identity()\n",
              "          (k_norm): Identity()\n",
              "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "          (proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
              "          (proj_drop): Identity()\n",
              "        )\n",
              "        (ls1): Identity()\n",
              "        (drop_path1): Identity()\n",
              "        (norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): Mlp(\n",
              "          (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
              "          (act): GELU(approximate='tanh')\n",
              "          (drop1): Dropout(p=0.0, inplace=False)\n",
              "          (norm): Identity()\n",
              "          (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
              "          (drop2): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ls2): Identity()\n",
              "        (drop_path2): Identity()\n",
              "      )\n",
              "      (18): Block(\n",
              "        (norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (qkv): Linear(in_features=1152, out_features=3456, bias=True)\n",
              "          (q_norm): Identity()\n",
              "          (k_norm): Identity()\n",
              "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "          (proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
              "          (proj_drop): Identity()\n",
              "        )\n",
              "        (ls1): Identity()\n",
              "        (drop_path1): Identity()\n",
              "        (norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): Mlp(\n",
              "          (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
              "          (act): GELU(approximate='tanh')\n",
              "          (drop1): Dropout(p=0.0, inplace=False)\n",
              "          (norm): Identity()\n",
              "          (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
              "          (drop2): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ls2): Identity()\n",
              "        (drop_path2): Identity()\n",
              "      )\n",
              "      (19): Block(\n",
              "        (norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (qkv): Linear(in_features=1152, out_features=3456, bias=True)\n",
              "          (q_norm): Identity()\n",
              "          (k_norm): Identity()\n",
              "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "          (proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
              "          (proj_drop): Identity()\n",
              "        )\n",
              "        (ls1): Identity()\n",
              "        (drop_path1): Identity()\n",
              "        (norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): Mlp(\n",
              "          (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
              "          (act): GELU(approximate='tanh')\n",
              "          (drop1): Dropout(p=0.0, inplace=False)\n",
              "          (norm): Identity()\n",
              "          (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
              "          (drop2): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ls2): Identity()\n",
              "        (drop_path2): Identity()\n",
              "      )\n",
              "      (20): Block(\n",
              "        (norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (qkv): Linear(in_features=1152, out_features=3456, bias=True)\n",
              "          (q_norm): Identity()\n",
              "          (k_norm): Identity()\n",
              "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "          (proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
              "          (proj_drop): Identity()\n",
              "        )\n",
              "        (ls1): Identity()\n",
              "        (drop_path1): Identity()\n",
              "        (norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): Mlp(\n",
              "          (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
              "          (act): GELU(approximate='tanh')\n",
              "          (drop1): Dropout(p=0.0, inplace=False)\n",
              "          (norm): Identity()\n",
              "          (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
              "          (drop2): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ls2): Identity()\n",
              "        (drop_path2): Identity()\n",
              "      )\n",
              "      (21): Block(\n",
              "        (norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (qkv): Linear(in_features=1152, out_features=3456, bias=True)\n",
              "          (q_norm): Identity()\n",
              "          (k_norm): Identity()\n",
              "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "          (proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
              "          (proj_drop): Identity()\n",
              "        )\n",
              "        (ls1): Identity()\n",
              "        (drop_path1): Identity()\n",
              "        (norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): Mlp(\n",
              "          (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
              "          (act): GELU(approximate='tanh')\n",
              "          (drop1): Dropout(p=0.0, inplace=False)\n",
              "          (norm): Identity()\n",
              "          (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
              "          (drop2): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ls2): Identity()\n",
              "        (drop_path2): Identity()\n",
              "      )\n",
              "      (22): Block(\n",
              "        (norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (qkv): Linear(in_features=1152, out_features=3456, bias=True)\n",
              "          (q_norm): Identity()\n",
              "          (k_norm): Identity()\n",
              "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "          (proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
              "          (proj_drop): Identity()\n",
              "        )\n",
              "        (ls1): Identity()\n",
              "        (drop_path1): Identity()\n",
              "        (norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): Mlp(\n",
              "          (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
              "          (act): GELU(approximate='tanh')\n",
              "          (drop1): Dropout(p=0.0, inplace=False)\n",
              "          (norm): Identity()\n",
              "          (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
              "          (drop2): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ls2): Identity()\n",
              "        (drop_path2): Identity()\n",
              "      )\n",
              "      (23): Block(\n",
              "        (norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (qkv): Linear(in_features=1152, out_features=3456, bias=True)\n",
              "          (q_norm): Identity()\n",
              "          (k_norm): Identity()\n",
              "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "          (proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
              "          (proj_drop): Identity()\n",
              "        )\n",
              "        (ls1): Identity()\n",
              "        (drop_path1): Identity()\n",
              "        (norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): Mlp(\n",
              "          (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
              "          (act): GELU(approximate='tanh')\n",
              "          (drop1): Dropout(p=0.0, inplace=False)\n",
              "          (norm): Identity()\n",
              "          (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
              "          (drop2): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ls2): Identity()\n",
              "        (drop_path2): Identity()\n",
              "      )\n",
              "      (24): Block(\n",
              "        (norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (qkv): Linear(in_features=1152, out_features=3456, bias=True)\n",
              "          (q_norm): Identity()\n",
              "          (k_norm): Identity()\n",
              "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "          (proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
              "          (proj_drop): Identity()\n",
              "        )\n",
              "        (ls1): Identity()\n",
              "        (drop_path1): Identity()\n",
              "        (norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): Mlp(\n",
              "          (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
              "          (act): GELU(approximate='tanh')\n",
              "          (drop1): Dropout(p=0.0, inplace=False)\n",
              "          (norm): Identity()\n",
              "          (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
              "          (drop2): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ls2): Identity()\n",
              "        (drop_path2): Identity()\n",
              "      )\n",
              "      (25): Block(\n",
              "        (norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (qkv): Linear(in_features=1152, out_features=3456, bias=True)\n",
              "          (q_norm): Identity()\n",
              "          (k_norm): Identity()\n",
              "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "          (proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
              "          (proj_drop): Identity()\n",
              "        )\n",
              "        (ls1): Identity()\n",
              "        (drop_path1): Identity()\n",
              "        (norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): Mlp(\n",
              "          (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
              "          (act): GELU(approximate='tanh')\n",
              "          (drop1): Dropout(p=0.0, inplace=False)\n",
              "          (norm): Identity()\n",
              "          (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
              "          (drop2): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ls2): Identity()\n",
              "        (drop_path2): Identity()\n",
              "      )\n",
              "      (26): Block(\n",
              "        (norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (qkv): Linear(in_features=1152, out_features=3456, bias=True)\n",
              "          (q_norm): Identity()\n",
              "          (k_norm): Identity()\n",
              "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "          (proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
              "          (proj_drop): Identity()\n",
              "        )\n",
              "        (ls1): Identity()\n",
              "        (drop_path1): Identity()\n",
              "        (norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): Mlp(\n",
              "          (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
              "          (act): GELU(approximate='tanh')\n",
              "          (drop1): Dropout(p=0.0, inplace=False)\n",
              "          (norm): Identity()\n",
              "          (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
              "          (drop2): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ls2): Identity()\n",
              "        (drop_path2): Identity()\n",
              "      )\n",
              "    )\n",
              "    (norm): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
              "    (attn_pool): AttentionPoolLatent(\n",
              "      (q): Linear(in_features=1152, out_features=1152, bias=True)\n",
              "      (kv): Linear(in_features=1152, out_features=2304, bias=True)\n",
              "      (q_norm): Identity()\n",
              "      (k_norm): Identity()\n",
              "      (proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
              "      (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      (norm): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
              "        (act): GELU(approximate='none')\n",
              "        (drop1): Dropout(p=0.0, inplace=False)\n",
              "        (norm): Identity()\n",
              "        (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
              "        (drop2): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (fc_norm): Identity()\n",
              "    (head_drop): Dropout(p=0.0, inplace=False)\n",
              "    (head): Identity()\n",
              "  )\n",
              "  (projector): MlpProjector(\n",
              "    (layers): Sequential(\n",
              "      (0): Linear(in_features=4608, out_features=1280, bias=True)\n",
              "      (1): GELU(approximate='none')\n",
              "      (2): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "    )\n",
              "  )\n",
              "  (language): DeepseekV2ForCausalLM(\n",
              "    (model): DeepseekV2Model(\n",
              "      (embed_tokens): Embedding(129280, 1280)\n",
              "      (layers): ModuleList(\n",
              "        (0): DeepseekV2DecoderLayer(\n",
              "          (self_attn): LlamaAttention(\n",
              "            (q_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
              "            (k_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
              "            (v_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
              "            (o_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
              "            (rotary_emb): LlamaRotaryEmbedding()\n",
              "          )\n",
              "          (mlp): DeepseekV2MLP(\n",
              "            (gate_proj): Linear(in_features=1280, out_features=6848, bias=False)\n",
              "            (up_proj): Linear(in_features=1280, out_features=6848, bias=False)\n",
              "            (down_proj): Linear(in_features=6848, out_features=1280, bias=False)\n",
              "            (act_fn): SiLU()\n",
              "          )\n",
              "          (input_layernorm): DeepseekV2RMSNorm()\n",
              "          (post_attention_layernorm): DeepseekV2RMSNorm()\n",
              "        )\n",
              "        (1-11): 11 x DeepseekV2DecoderLayer(\n",
              "          (self_attn): LlamaAttention(\n",
              "            (q_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
              "            (k_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
              "            (v_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
              "            (o_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
              "            (rotary_emb): LlamaRotaryEmbedding()\n",
              "          )\n",
              "          (mlp): DeepseekV2MoE(\n",
              "            (experts): ModuleList(\n",
              "              (0-63): 64 x DeepseekV2MLP(\n",
              "                (gate_proj): Linear(in_features=1280, out_features=896, bias=False)\n",
              "                (up_proj): Linear(in_features=1280, out_features=896, bias=False)\n",
              "                (down_proj): Linear(in_features=896, out_features=1280, bias=False)\n",
              "                (act_fn): SiLU()\n",
              "              )\n",
              "            )\n",
              "            (gate): MoEGate()\n",
              "            (shared_experts): DeepseekV2MLP(\n",
              "              (gate_proj): Linear(in_features=1280, out_features=1792, bias=False)\n",
              "              (up_proj): Linear(in_features=1280, out_features=1792, bias=False)\n",
              "              (down_proj): Linear(in_features=1792, out_features=1280, bias=False)\n",
              "              (act_fn): SiLU()\n",
              "            )\n",
              "          )\n",
              "          (input_layernorm): DeepseekV2RMSNorm()\n",
              "          (post_attention_layernorm): DeepseekV2RMSNorm()\n",
              "        )\n",
              "      )\n",
              "      (norm): DeepseekV2RMSNorm()\n",
              "    )\n",
              "    (lm_head): Linear(in_features=1280, out_features=129280, bias=False)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_id = \"deepseek-ai/deepseek-vl2-tiny\"\n",
        "\n",
        "processor = DeepseekVLV2Processor.from_pretrained(model_id)\n",
        "\n",
        "model = DeepseekVLV2ForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "model.eval()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-12-18T17:04:52.900004Z",
          "iopub.status.busy": "2025-12-18T17:04:52.899255Z",
          "iopub.status.idle": "2025-12-18T17:04:55.044136Z",
          "shell.execute_reply": "2025-12-18T17:04:55.043342Z",
          "shell.execute_reply.started": "2025-12-18T17:04:52.899978Z"
        },
        "trusted": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.9.0)\n",
            "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.11/dist-packages (from accelerate) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (25.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (7.1.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate) (6.0.3)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.0.1+cu118)\n",
            "Requirement already satisfied: huggingface_hub>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.36.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.5.3)\n",
            "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<3.0.0,>=1.17->accelerate) (1.3.8)\n",
            "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<3.0.0,>=1.17->accelerate) (1.2.4)\n",
            "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<3.0.0,>=1.17->accelerate) (0.1.1)\n",
            "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<3.0.0,>=1.17->accelerate) (2025.3.0)\n",
            "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<3.0.0,>=1.17->accelerate) (2022.3.0)\n",
            "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<3.0.0,>=1.17->accelerate) (2.4.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.21.0->accelerate) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.21.0->accelerate) (2025.10.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.21.0->accelerate) (2.32.5)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.21.0->accelerate) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.21.0->accelerate) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.21.0->accelerate) (1.2.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.11/dist-packages (from triton==2.0.0->torch>=2.0.0->accelerate) (3.31.6)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.11/dist-packages (from triton==2.0.0->torch>=2.0.0->accelerate) (15.0.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.3)\n",
            "Requirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3.0.0,>=1.17->accelerate) (2025.3.0)\n",
            "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3.0.0,>=1.17->accelerate) (2024.2.0)\n",
            "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3.0.0,>=1.17->accelerate) (2022.3.0)\n",
            "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<3.0.0,>=1.17->accelerate) (2024.2.0)\n",
            "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<3.0.0,>=1.17->accelerate) (1.4.0)\n",
            "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<3.0.0,>=1.17->accelerate) (2024.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2025.10.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch>=2.0.0->accelerate) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-12-18T17:53:55.415097Z",
          "iopub.status.busy": "2025-12-18T17:53:55.414773Z",
          "iopub.status.idle": "2025-12-18T17:53:55.420678Z",
          "shell.execute_reply": "2025-12-18T17:53:55.419840Z",
          "shell.execute_reply.started": "2025-12-18T17:53:55.415075Z"
        },
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prompt đã được định nghĩa\n"
          ]
        }
      ],
      "source": [
        "# Import các thư viện cần thiết\n",
        "from PIL import Image\n",
        "import json\n",
        "import re\n",
        "\n",
        "# Định nghĩa prompt - Sử dụng prompt đã được tối ưu (không có placeholder)\n",
        "prompt = \"\"\"Bạn là chuyên gia phân tích tài liệu tiếng Việt. Nhiệm vụ của bạn là nhận diện và định vị CON DẤU (seal/stamp) và CHỮ KÝ (signature) trong hình ảnh tài liệu.\n",
        "\n",
        "CON DẤU thường có đặc điểm:\n",
        "- Hình tròn hoặc hình chữ nhật\n",
        "- Màu đỏ hoặc màu khác nổi bật hoặc đen thì là bản sao (copy)\n",
        "- Có văn bản bên trong (tên cơ quan, tổ chức)\n",
        "- Thường ở góc trên bên phải hoặc gần chữ ký\n",
        "\n",
        "CHỮ KÝ thường có đặc điểm:\n",
        "- Nét viết tay, đường nét cong\n",
        "- Màu đen hoặc xanh\n",
        "- Thường ở góc dưới bên phải\n",
        "- Có thể kèm theo tên người ký\n",
        "\n",
        "Hãy phân tích hình ảnh và trả về CHỈ JSON, không có markdown, không có giải thích.\n",
        "\n",
        "QUAN TRỌNG: bbox phải là mảng 4 số nguyên [x1, y1, x2, y2] với:\n",
        "- x1, y1: tọa độ góc trên bên trái (pixel)\n",
        "- x2, y2: tọa độ góc dưới bên phải (pixel)\n",
        "- Tất cả giá trị phải là số nguyên thực tế, KHÔNG phải placeholder như \"number x0\", \"double\", \"str(...)\"\n",
        "\n",
        "Schema JSON bắt buộc:\n",
        "{\n",
        "  \"page\": 0,\n",
        "  \"width\": 1200,\n",
        "  \"height\": 1600,\n",
        "  \"seals\": [\n",
        "    {\n",
        "      \"type\": \"seal\",\n",
        "      \"bbox\": [100, 50, 300, 200],\n",
        "      \"confidence\": 0.95,\n",
        "      \"description\": \"Con dấu UBND\"\n",
        "    }\n",
        "  ],\n",
        "  \"signatures\": [\n",
        "    {\n",
        "      \"type\": \"signature\",\n",
        "      \"bbox\": [800, 1400, 1100, 1550],\n",
        "      \"confidence\": 0.90,\n",
        "      \"description\": \"Chữ ký người đại diện\"\n",
        "    }\n",
        "  ]\n",
        "}\n",
        "\n",
        "Ví dụ: Nếu con dấu ở vị trí từ pixel (100, 50) đến (300, 200), thì bbox = [100, 50, 300, 200]\n",
        "\n",
        "Trả về CHỈ JSON object với các số thực tế, không có gì khác.\"\"\"\n",
        "\n",
        "print(\"Prompt đã được định nghĩa\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-12-18T17:55:08.359714Z",
          "iopub.status.busy": "2025-12-18T17:55:08.359119Z",
          "iopub.status.idle": "2025-12-18T17:55:08.715520Z",
          "shell.execute_reply": "2025-12-18T17:55:08.714797Z",
          "shell.execute_reply.started": "2025-12-18T17:55:08.359690Z"
        },
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Đã load hình ảnh: /kaggle/input/signatures-and-stamps/1/JfOEcp8fLGNH.jpg\n",
            "Kích thước: 1653 x 2336\n"
          ]
        }
      ],
      "source": [
        "# Load hình ảnh\n",
        "image_path = \"/kaggle/input/signatures-and-stamps/1/JfOEcp8fLGNH.jpg\"\n",
        "image = Image.open(image_path).convert(\"RGB\")\n",
        "\n",
        "# Lấy kích thước hình ảnh\n",
        "img_width, img_height = image.size\n",
        "print(f\"Đã load hình ảnh: {image_path}\")\n",
        "print(f\"Kích thước: {img_width} x {img_height}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-12-18T17:55:10.640171Z",
          "iopub.status.busy": "2025-12-18T17:55:10.639778Z",
          "iopub.status.idle": "2025-12-18T17:55:10.817072Z",
          "shell.execute_reply": "2025-12-18T17:55:10.816281Z",
          "shell.execute_reply.started": "2025-12-18T17:55:10.640147Z"
        },
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Đã chuẩn bị inputs cho model\n",
            "Input keys: ['input_ids', 'attention_mask', 'images', 'images_seq_mask', 'images_spatial_crop']\n",
            "Input shapes và dtypes:\n",
            "  input_ids: shape=torch.Size([1, 2464]), dtype=torch.int64\n",
            "  attention_mask: shape=torch.Size([1, 2464]), dtype=torch.bool\n",
            "  images: shape=torch.Size([1, 10, 3, 384, 384]), dtype=torch.float16\n",
            "  images_seq_mask: shape=torch.Size([1, 2464]), dtype=torch.bool\n",
            "  images_spatial_crop: shape=torch.Size([1, 1, 2]), dtype=torch.int64\n"
          ]
        }
      ],
      "source": [
        "# Chuẩn bị input cho model\n",
        "# Format theo DeepSeek VL2 conversation format\n",
        "from deepseek_vl2.utils.io import load_pil_images\n",
        "\n",
        "conversation = [\n",
        "    {\n",
        "        \"role\": \"<|User|>\",\n",
        "        \"content\": f\"<image>\\n{prompt}\",\n",
        "        \"images\": [image_path]  # Path to image\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"<|Assistant|>\",\n",
        "        \"content\": \"\"\n",
        "    }\n",
        "]\n",
        "\n",
        "# Load images và process conversation\n",
        "pil_images = load_pil_images(conversation)\n",
        "\n",
        "# Process inputs - trả về BatchCollateOutput object\n",
        "batch_output = processor(\n",
        "    conversations=conversation,\n",
        "    images=pil_images,\n",
        "    force_batchify=True,\n",
        "    system_prompt=\"\"\n",
        ")\n",
        "\n",
        "# Move các tensors lên device và convert dtype\n",
        "# BatchCollateOutput có các attributes: input_ids, attention_mask, images, images_seq_mask, etc.\n",
        "device = next(model.parameters()).device\n",
        "dtype = next(model.parameters()).dtype  # Lấy dtype từ model (float16)\n",
        "\n",
        "# Tạo dictionary từ các attributes của BatchCollateOutput\n",
        "prepare_inputs = {}\n",
        "if hasattr(batch_output, 'input_ids'):\n",
        "    prepare_inputs['input_ids'] = batch_output.input_ids.to(device)\n",
        "if hasattr(batch_output, 'attention_mask'):\n",
        "    prepare_inputs['attention_mask'] = batch_output.attention_mask.to(device)\n",
        "if hasattr(batch_output, 'images'):\n",
        "    # Quan trọng: convert images sang cùng dtype với model (float16)\n",
        "    prepare_inputs['images'] = batch_output.images.to(device=device, dtype=dtype)\n",
        "if hasattr(batch_output, 'images_seq_mask'):\n",
        "    prepare_inputs['images_seq_mask'] = batch_output.images_seq_mask.to(device)\n",
        "if hasattr(batch_output, 'images_spatial_crop'):\n",
        "    prepare_inputs['images_spatial_crop'] = batch_output.images_spatial_crop\n",
        "\n",
        "print(\"Đã chuẩn bị inputs cho model\")\n",
        "print(f\"Input keys: {list(prepare_inputs.keys())}\")\n",
        "print(f\"Input shapes và dtypes:\")\n",
        "for k, v in prepare_inputs.items():\n",
        "    if hasattr(v, 'shape') and hasattr(v, 'dtype'):\n",
        "        print(f\"  {k}: shape={v.shape}, dtype={v.dtype}\")\n",
        "    elif hasattr(v, 'shape'):\n",
        "        print(f\"  {k}: shape={v.shape}\")\n",
        "    else:\n",
        "        print(f\"  {k}: type={type(v)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Parse response và visualize kết quả\n",
        "import os\n",
        "from datetime import datetime\n",
        "from PIL import ImageDraw\n",
        "\n",
        "# Thử import từ utils nếu có thể (nếu đang chạy từ module structure)\n",
        "try:\n",
        "    import sys\n",
        "    sys.path.append('/kaggle/working')\n",
        "    # Thử import từ module nếu có\n",
        "    try:\n",
        "        from src.modules.signatures_and_stamps.utils import parse_response, draw_boxes\n",
        "        USE_UTILS = True\n",
        "    except ImportError:\n",
        "        USE_UTILS = False\n",
        "except:\n",
        "    USE_UTILS = False\n",
        "\n",
        "try:\n",
        "    if USE_UTILS:\n",
        "        # Sử dụng parse_response và draw_boxes từ utils\n",
        "        result = parse_response(response, img_width=img_width, img_height=img_height)\n",
        "        annotated_image = draw_boxes(image.copy(), result, seal_color=\"red\", signature_color=\"blue\")\n",
        "    else:\n",
        "        # Parse JSON trực tiếp và vẽ boxes\n",
        "        # Clean response để loại bỏ markdown\n",
        "        cleaned_response = response.strip()\n",
        "        json_match = re.search(r'\\{.*\\}', cleaned_response, re.DOTALL)\n",
        "        \n",
        "        if json_match:\n",
        "            data = json.loads(json_match.group(0))\n",
        "            \n",
        "            # Vẽ boxes lên ảnh\n",
        "            annotated_image = image.copy()\n",
        "            draw = ImageDraw.Draw(annotated_image)\n",
        "            \n",
        "            num_seals = 0\n",
        "            num_signatures = 0\n",
        "            \n",
        "            # Vẽ seals (màu đỏ)\n",
        "            if \"seals\" in data and isinstance(data[\"seals\"], list):\n",
        "                for seal in data[\"seals\"]:\n",
        "                    if \"bbox\" in seal and isinstance(seal[\"bbox\"], list) and len(seal[\"bbox\"]) == 4:\n",
        "                        try:\n",
        "                            bbox = seal[\"bbox\"]\n",
        "                            # Skip nếu có placeholder\n",
        "                            bbox_str = str(bbox).lower()\n",
        "                            if any(ph in bbox_str for ph in [\"x0\", \"y0\", \"number\", \"of seal\", \"of signature\"]):\n",
        "                                continue\n",
        "                            coords = [int(float(x)) for x in bbox]\n",
        "                            if coords[2] > coords[0] and coords[3] > coords[1]:\n",
        "                                # Normalize coordinates\n",
        "                                x1 = max(0, min(coords[0], img_width - 1))\n",
        "                                y1 = max(0, min(coords[1], img_height - 1))\n",
        "                                x2 = max(x1 + 1, min(coords[2], img_width))\n",
        "                                y2 = max(y1 + 1, min(coords[3], img_height))\n",
        "                                draw.rectangle([x1, y1, x2, y2], outline=\"red\", width=3)\n",
        "                                draw.text((x1, y1 - 20), \"SEAL\", fill=\"red\")\n",
        "                                num_seals += 1\n",
        "                        except (ValueError, TypeError):\n",
        "                            pass\n",
        "            \n",
        "            # Vẽ signatures (màu xanh)\n",
        "            if \"signatures\" in data and isinstance(data[\"signatures\"], list):\n",
        "                for sig in data[\"signatures\"]:\n",
        "                    if \"bbox\" in sig and isinstance(sig[\"bbox\"], list) and len(sig[\"bbox\"]) == 4:\n",
        "                        try:\n",
        "                            bbox = sig[\"bbox\"]\n",
        "                            # Skip nếu có placeholder\n",
        "                            bbox_str = str(bbox).lower()\n",
        "                            if any(ph in bbox_str for ph in [\"x0\", \"y0\", \"number\", \"of seal\", \"of signature\"]):\n",
        "                                continue\n",
        "                            coords = [int(float(x)) for x in bbox]\n",
        "                            if coords[2] > coords[0] and coords[3] > coords[1]:\n",
        "                                # Normalize coordinates\n",
        "                                x1 = max(0, min(coords[0], img_width - 1))\n",
        "                                y1 = max(0, min(coords[1], img_height - 1))\n",
        "                                x2 = max(x1 + 1, min(coords[2], img_width))\n",
        "                                y2 = max(y1 + 1, min(coords[3], img_height))\n",
        "                                draw.rectangle([x1, y1, x2, y2], outline=\"blue\", width=3)\n",
        "                                draw.text((x1, y1 - 20), \"SIGNATURE\", fill=\"blue\")\n",
        "                                num_signatures += 1\n",
        "                        except (ValueError, TypeError):\n",
        "                            pass\n",
        "            \n",
        "            if USE_UTILS:\n",
        "                num_seals = len(result.seals)\n",
        "                num_signatures = len(result.signatures)\n",
        "        else:\n",
        "            print(\"⚠️  Không tìm thấy JSON trong response\")\n",
        "            annotated_image = image.copy()\n",
        "            num_seals = 0\n",
        "            num_signatures = 0\n",
        "    \n",
        "    # Lưu ảnh vào /kaggle/working/\n",
        "    output_dir = \"/kaggle/working\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    \n",
        "    # Tạo tên file với timestamp\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    if 'image_path' in locals():\n",
        "        image_name = os.path.basename(image_path).rsplit('.', 1)[0]\n",
        "    else:\n",
        "        image_name = \"detected\"\n",
        "    output_path = os.path.join(output_dir, f\"{image_name}_detected_{timestamp}.jpg\")\n",
        "    \n",
        "    annotated_image.save(output_path)\n",
        "    \n",
        "    # In thông tin\n",
        "    print(f\"\\n✅ Đã lưu ảnh đã được đánh dấu vào: {output_path}\")\n",
        "    print(f\"📊 Phát hiện: {num_seals} con dấu, {num_signatures} chữ ký\")\n",
        "    \n",
        "    # Hiển thị ảnh (nếu trong môi trường hỗ trợ display)\n",
        "    try:\n",
        "        from IPython.display import display\n",
        "        display(annotated_image)\n",
        "    except:\n",
        "        print(f\"💡 Ảnh đã được lưu tại: {output_path}\")\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"❌ Lỗi khi parse và visualize: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-12-18T17:55:14.458318Z",
          "iopub.status.busy": "2025-12-18T17:55:14.457613Z",
          "iopub.status.idle": "2025-12-18T17:55:20.553242Z",
          "shell.execute_reply": "2025-12-18T17:55:20.552437Z",
          "shell.execute_reply.started": "2025-12-18T17:55:14.458292Z"
        },
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "  \"page\": 1,\n",
            "  \"width\": 800,\n",
            "  \"height\": 600,\n",
            "  \"seals\": [\n",
            "    {\n",
            "      \"type\": \"seal\",\n",
            "      \"bbox\": [100, 100, 300, 300],\n",
            "      \"confidence\": double,\n",
            "      \"description\": str (\"Name of certifying person/organization\")\n",
            "    }\n",
            "  ],\n",
            "  \"signatures\": [\n",
            "    {\n",
            "      \"type\": \"signature\",\n",
            "      \"bbox\": [100, 100, 300, 300],\n",
            "      \"confidence\": double,\n",
            "      \"description\": str (\"Name of the person/organization confirming the signature\")\n",
            "    }\n",
            "  ]\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "# Generate với model\n",
        "# Giảm max_new_tokens và thêm early stopping để tránh generate quá nhiều\n",
        "with torch.no_grad():\n",
        "    outputs = model.generate(\n",
        "        **prepare_inputs,\n",
        "        max_new_tokens=512,  # Giảm từ 2048 xuống 512\n",
        "        do_sample=False,\n",
        "        pad_token_id=processor.tokenizer.pad_token_id,\n",
        "        eos_token_id=processor.tokenizer.eos_token_id,\n",
        "    )\n",
        "\n",
        "# Decode output - chỉ decode phần response mới (không bao gồm input)\n",
        "input_ids = prepare_inputs[\"input_ids\"]\n",
        "generated_ids = outputs[0][input_ids.shape[1]:]\n",
        "\n",
        "# Decode chỉ phần generated\n",
        "response = processor.tokenizer.decode(\n",
        "    generated_ids,\n",
        "    skip_special_tokens=True,\n",
        "    clean_up_tokenization_spaces=False\n",
        ")\n",
        "print(response)\n",
        "# Tìm vị trí kết thúc JSON hợp lệ - dừng khi gặp ký tự đặc biệt\n",
        "# stop_chars = ['<cell', '<await', '<lineno', 'import)', '%', '\"{']\n",
        "# stop_pos = len(response)\n",
        "# for char in stop_chars:\n",
        "#     pos = response.find(char)\n",
        "#     if pos != -1 and pos < stop_pos:\n",
        "#         stop_pos = pos\n",
        "\n",
        "# # Chỉ lấy phần response hợp lệ\n",
        "# response = response[:stop_pos].strip()\n",
        "\n",
        "# print(\"Đã generate response từ model\")\n",
        "# print(\"\\n\" + \"=\"*50)\n",
        "# print(\"RAW RESPONSE (cleaned):\")\n",
        "# print(\"=\"*50)\n",
        "# print(response)"
      ]
    }
  ],
  "metadata": {
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "datasetId": 3705714,
          "isSourceIdPinned": false,
          "sourceId": 6423615,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 9044358,
          "sourceId": 14185864,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 31193,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
